{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a model Tensorflow/Keras using Amazon SageMaker\n",
    "\n",
    "In this blog post, weâ€™ll demonstrate how to train a model by TensorFlow/Keras using Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Set up\n",
    "\n",
    "After create a new notebook instance, set the kernel to conda_tensorflow_p36.\n",
    "\n",
    "The get_execution_role function retrieves the AWS Identity and Access Management (IAM) role you created at the time of creating your notebook instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, re\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--epochs', type=int, default=10)\n",
    "    parser.add_argument('--batch-size', type=int, default=128)\n",
    "    parser.add_argument('--learning-rate', type=float, default=0.1)\n",
    "    parser.add_argument('--gpu-count', type=int, default=os.environ['SM_NUM_GPUS'])\n",
    "    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "    epochs = args.epochs\n",
    "    learning_rate = args.learning_rate\n",
    "    batch_size = args.batch_size\n",
    "    gpu_count = args.gpu_count\n",
    "    model_dir = args.model_dir\n",
    "\n",
    "    # Load mnist from tf.keras.datasets.mnist or load data from csv\n",
    "    # if size of csv is small, it can be uploaded to the notebook otherwise upload it to the s3\n",
    "    # training_data = pd.read_csv('train.csv')\n",
    "\n",
    "    mnist = tf.keras.datasets.mnist\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train = x_train / 255\n",
    "    x_test = x_test / 255\n",
    "    y_train = keras.utils.to_categorical(y_train, 10)\n",
    "    y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(28, 28)))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    # loss, optimizers\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.SGD(lr=learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "    tf.saved_model.simple_save(\n",
    "        tf.keras.backend.get_session(),\n",
    "        os.path.join(model_dir, '1'),\n",
    "        inputs={'inputs': model.input},\n",
    "        outputs={t.name: t for t in model.outputs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "model = TensorFlow(\n",
    "    entry_point='train.py', train_instance_type='ml.m4.xlarge',\n",
    "    train_instance_count=1, role=role,\n",
    "    base_job_name='mnist', framework_version='1.15',\n",
    "    py_version='py3', script_mode=True\n",
    ")\n",
    "\n",
    "model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Deploy the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploy step takes a long time, after that, sagemaker model and endpoint will be created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "predictor = model.deploy(initial_instance_count=1,\n",
    "                                   instance_type='ml.m4.xlarge', endpoint_name='endpoint-name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Invoke the endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Invoke the SageMaker endpoint from the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = predictor.endpoint\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.tensorflow.model import TensorFlowModel\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "predictor = sagemaker.tensorflow.model.TensorFlowPredictor(endpoint_name, sagemaker_session)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "data = np.random.randn(1, 28, 28)\n",
    "\n",
    "predictor.predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Invoke the SageMaker endpoint using a boto3 client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import numpy as np\n",
    "import io\n",
    " \n",
    "client = boto3.client('runtime.sagemaker')\n",
    "\n",
    "data = np.random.randn(1, 28, 28).tolist()\n",
    "\n",
    "response = client.invoke_endpoint(EndpointName=endpoint_name, Body=json.dumps(data), ContentType='application/json')\n",
    "\n",
    "response_body = response['Body'].read()\n",
    "\n",
    "data = json.loads(response_body)['predictions']\n",
    "\n",
    "value = np.argmax(data)\n",
    "\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5. Clean up\n",
    "\n",
    "To avoid incurring unnecessary charges, use the AWS Management Console to delete the resources that you created for this exercise:\n",
    "https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-cleanup.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
